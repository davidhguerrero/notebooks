{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://export.arxiv.org/api/query?search_query=cat:cs.CV&max_results=100\n",
      "182881 characters retrieved\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "s_q = 'search_query=cat:cs.CV'\n",
    "m_r = 'max_results=100'\n",
    "url = 'http://export.arxiv.org/api/query?'+ s_q + '&' + m_r\n",
    "print url\n",
    "data = urllib.urlopen(url).read()\n",
    "print str(len(data)) + ' characters retrieved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1998', u'1998', u'1999', u'2000', u'2000', u'2000', u'2000', u'2000', u'2000', u'2000', u'2000', u'2002', u'2003', u'2003', u'2003', u'2003', u'2003', u'2003', u'2003', u'2003', u'2003', u'2003', u'2004', u'2004', u'2004', u'2004', u'2004', u'2004', u'2005', u'2005', u'2005', u'2005', u'2005', u'2005', u'2005', u'2005', u'2005', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2006', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2007', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008', u'2008']\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "feed = feedparser.parse(data)\n",
    "#print str(len(feed.entries))\n",
    "\n",
    "#print feed.entries[0].title\n",
    "#print feed.entries[0].summary\n",
    "#print feed.entries[0].published\n",
    "\n",
    "dates = list()\n",
    "abstracts = list()\n",
    "#for x in range(0, str(len(feed.entries))):\n",
    "for entry in feed.entries:\n",
    "    #print entry\n",
    "    dates.append(entry.published[0:4])\n",
    "    abstracts.append(entry.summary) \n",
    "print dates\n",
    "#print feed.entries[0]\n",
    "    #dates = feed.entries[x].published\n",
    "    #abstracts = feed.entries[x].summary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código que extrae las \"bag of words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (23, 1), (29, 2), (36, 2), (43, 1), (45, 1), (49, 1), (60, 1), (67, 1), (74, 2), (77, 1), (100, 1), (110, 1), (113, 3), (117, 1), (127, 1), (137, 1), (148, 3), (166, 1), (172, 1), (178, 1), (182, 1), (217, 1), (249, 2), (251, 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntotal_corpus_bow.append(corpus_bow)\\ntotal_corpus_bow\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import nltk.stem\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora \n",
    "\n",
    "import urllib\n",
    "\n",
    "s_q = 'search_query=cat:cs.CV'\n",
    "m_r = 'max_results=10'\n",
    "url = 'http://export.arxiv.org/api/query?'+ s_q + '&' + m_r\n",
    "data = urllib.urlopen(url).read()\n",
    "\n",
    "dates = list()\n",
    "abstracts=list()\n",
    "tokenize_text=list()\n",
    "clean_text=list()\n",
    "clean_abstracts=list()\n",
    "clean_abstracts_bow=list()\n",
    "clean_entry=list()\n",
    "total_corpus_bow=list()\n",
    "\n",
    "\n",
    "s=nltk.stem.SnowballStemmer('english')\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "feed = feedparser.parse(data)\n",
    "feed_entry = len(feed.entries)\n",
    "#print feed_entry\n",
    "\n",
    "for entry in feed.entries:  \n",
    "    text = entry.summary #Read 1st summary\n",
    "    dates.append(entry.published[0:4]) #Create a list with years\n",
    "    tokens = word_tokenize(text) #Split in words\n",
    "    for token in tokens:\n",
    "        if token.isalnum():\n",
    "            tokenize_text.append(token.lower()) #Remove capital letters of each word\n",
    "            stem_token = s.stem(token) #Select one word\n",
    "            if stem_token not in eng_stopwords: #if not match in stop words\n",
    "                clean_text.append(stem_token)\n",
    "    \n",
    "    #clean_text = ' '.join(clean_text) \n",
    "    clean_abstracts.append(clean_text)\n",
    "    clean_abstracts_bow.extend(clean_text) #lista de las word bags de todos los textos\n",
    "    #n= n + 1\n",
    "    #print clean_text\n",
    "    #print \"-------\"\n",
    "    clean_text=list()\n",
    "\n",
    "\n",
    "#print clean_abstracts_bow\n",
    "D=gensim.corpora.Dictionary([clean_abstracts_bow])\n",
    "corpus_bow = D.doc2bow(clean_abstracts[0])\n",
    "print corpus_bow\n",
    "\"\"\"\n",
    "total_corpus_bow.append(corpus_bow)\n",
    "total_corpus_bow\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#clean_abstracts = ' '.join(clean_abstracts)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Funcion que convierte la lista en vector con el tamaño del diccionario, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (23, 1), (29, 2), (36, 2), (43, 1), (45, 1), (49, 1), (60, 1), (67, 1), (74, 2), (77, 1), (100, 1), (110, 1), (113, 3), (117, 1), (127, 1), (137, 1), (148, 3), (166, 1), (172, 1), (178, 1), (182, 1), (217, 1), (249, 2), (251, 1)]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector = [0] * len(D)\n",
    "def get_vector_corpus(list_corpus):\n",
    "        corpus = np.asarray(list_corpus)\n",
    "        index = corpus[:,0]\n",
    "        value = corpus[:,1]\n",
    "        for n in range (0, len(corpus)):\n",
    "            vector[index[n]] = value[n]\n",
    "\n",
    "get_vector_corpus(corpus_bow)\n",
    "print corpus_bow\n",
    "print vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary contains 10 tokens\n",
      "First tokens in the dictionary: \n",
      "0: algorithm use letter dataset uci repositori classifi distort raster imag english alphabet contrast rather complex network found produc compar better classif effici complex problem\n",
      "1: paper present invari scale linear bright chang invari base differenti therefor local featur rotate invari differenti gaussian oper third order propos implement invari perform analyz simul camera\n",
      "2: bayesian classifi differ attribut valu discuss use four popular dataset uci repositori interest featur network illustr network suitabl classif problem\n",
      "3: paper present invari gamma correct similar transform invari local featur base differenti implement use deriv gaussian use propos invari represent shown yield improv correl result templat match scenario\n",
      "4: fractal imag compress culik imag compress zerotre predict code wavelet imag decomposit coeffici succeed onli becaus typic imag compress possess signific degre besid common concept method turn even tight relat point algorithm reduc one techniqu anoth goal present paper demonstr relat paper offer interpret culik imag compress regular imag process term without resort finit state machin similar lofti languag interpret shown algorithm relat fractal imag compress method exact transform culik imag code use transform prove part imag ani zero wavelet coeffici root zerotre branch paper discuss zerotre code coeffici common appli vertic differ layer multiresolut decomposit rather within view interpret lead insight evolut imag compress techniqu causal predict predict wavelet decomposit among causal predict culik method\n",
      "5: paper establish verifi theori prevail wide among imag pattern recognit specialist indirect region match process stabl robust global match process concentr type nois repres clutter outlier occlus imageri demonstr analyz effect concentr nois typic decis make process simplifi two candid vote model theorem establish lower bound critic breakdown point elect decis result match process greater exact bound global match process impli former region process capabl accommod higher level nois latter global process befor result decis overturn present convinc experiment verif support onli theori flag recognit problem presenc local nois also valid conjectur facial recognit problem theorem remain valid decis make process involv import transform princip compon analysi gabor transform\n",
      "6: describ simpl effici algorithm generat dilat contour bilevel imag initi part contour extract explain good candid parallel comput code generat remaind algorithm linear natur\n",
      "7: give systemat abstract formul imag normal method appli general group imag transform illustr abstract analysi appli hierarchi view transform planar object\n",
      "8: paper present multiscal decomposit algorithm unlik standard wavelet transform propos oper linear shift invari central idea obtain shift invari averag align wavelet transform project circular shift signal shown transform obtain linear filter bank\n",
      "9: work deal video index viewpoint analysi compress video consid possibl applic motion analysi move object detect assist move object index summaris video allow imag motion queri propos approach base interest point first result test compar stabil differ type interest point detector compress sequenc\n"
     ]
    }
   ],
   "source": [
    "#Start Vectorization. 2.4 of TM1_NLP_student\n",
    "\n",
    "#print clean_abstracts\n",
    "from gensim import corpora\n",
    "#Adopto la nomenclatura de TM1_NLP_student\n",
    "corpus_clean = clean_abstracts \n",
    "D = gensim.corpora.Dictionary([corpus_clean])\n",
    "n_tokens = len(D)\n",
    "print \"The dictionary contains {0} tokens\".format(n_tokens)\n",
    "print \"First tokens in the dictionary: \"\n",
    "#Solo muestra los diez primeros\n",
    "for n in range(10):\n",
    "    print str(n) + \": \" + D[n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(10 unique tokens: [u'algorithm use letter dataset uci repositori classifi distort raster imag english alphabet contrast rather complex network found produc compar better classif effici complex problem', u'paper present invari scale linear bright chang invari base differenti therefor local featur rotate invari differenti gaussian oper third order propos implement invari perform analyz simul camera', u'bayesian classifi differ attribut valu discuss use four popular dataset uci repositori interest featur network illustr network suitabl classif problem', u'paper present invari gamma correct similar transform invari local featur base differenti implement use deriv gaussian use propos invari represent shown yield improv correl result templat match scenario', u'fractal imag compress culik imag compress zerotre predict code wavelet imag decomposit coeffici succeed onli becaus typic imag compress possess signific degre besid common concept method turn even tight relat point algorithm reduc one techniqu anoth goal present paper demonstr relat paper offer interpret culik imag compress regular imag process term without resort finit state machin similar lofti languag interpret shown algorithm relat fractal imag compress method exact transform culik imag code use transform prove part imag ani zero wavelet coeffici root zerotre branch paper discuss zerotre code coeffici common appli vertic differ layer multiresolut decomposit rather within view interpret lead insight evolut imag compress techniqu causal predict predict wavelet decomposit among causal predict culik method']...)\n",
      "[u'paper present multiscal decomposit algorithm unlik standard wavelet transform propos oper linear shift invari central idea obtain shift invari averag align wavelet transform project circular shift signal shown transform obtain linear filter bank', u'give systemat abstract formul imag normal method appli general group imag transform illustr abstract analysi appli hierarchi view transform planar object', u'paper present invari scale linear bright chang invari base differenti therefor local featur rotate invari differenti gaussian oper third order propos implement invari perform analyz simul camera', u'describ simpl effici algorithm generat dilat contour bilevel imag initi part contour extract explain good candid parallel comput code generat remaind algorithm linear natur', u'fractal imag compress culik imag compress zerotre predict code wavelet imag decomposit coeffici succeed onli becaus typic imag compress possess signific degre besid common concept method turn even tight relat point algorithm reduc one techniqu anoth goal present paper demonstr relat paper offer interpret culik imag compress regular imag process term without resort finit state machin similar lofti languag interpret shown algorithm relat fractal imag compress method exact transform culik imag code use transform prove part imag ani zero wavelet coeffici root zerotre branch paper discuss zerotre code coeffici common appli vertic differ layer multiresolut decomposit rather within view interpret lead insight evolut imag compress techniqu causal predict predict wavelet decomposit among causal predict culik method', u'paper present invari gamma correct similar transform invari local featur base differenti implement use deriv gaussian use propos invari represent shown yield improv correl result templat match scenario', u'work deal video index viewpoint analysi compress video consid possibl applic motion analysi move object detect assist move object index summaris video allow imag motion queri propos approach base interest point first result test compar stabil differ type interest point detector compress sequenc', u'paper establish verifi theori prevail wide among imag pattern recognit specialist indirect region match process stabl robust global match process concentr type nois repres clutter outlier occlus imageri demonstr analyz effect concentr nois typic decis make process simplifi two candid vote model theorem establish lower bound critic breakdown point elect decis result match process greater exact bound global match process impli former region process capabl accommod higher level nois latter global process befor result decis overturn present convinc experiment verif support onli theori flag recognit problem presenc local nois also valid conjectur facial recognit problem theorem remain valid decis make process involv import transform princip compon analysi gabor transform', u'bayesian classifi differ attribut valu discuss use four popular dataset uci repositori interest featur network illustr network suitabl classif problem', u'algorithm use letter dataset uci repositori classifi distort raster imag english alphabet contrast rather complex network found produc compar better classif effici complex problem']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print D\n",
    "D.doc2bow(corpus_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpusname = 'arxiv_v0'\n",
    "\n",
    "# We first sort the lists according to dates by zipping the lists together, and then unzipping after sorting\n",
    "zipped_list = zip(dates, clean_abstracts)\n",
    "zipped_list.sort()\n",
    "\n",
    "dates = [el[0] for el in zipped_list]\n",
    "clean_abstracts = [el[1] for el in zipped_list]\n",
    "\n",
    "# We create the file with the corpus\n",
    "f = open(corpusname+'_corpus.txt', 'wb')\n",
    "for abstract in clean_abstracts:\n",
    "    f.write(abstract+'\\n')\n",
    "f.close()\n",
    "\n",
    "# We create the file for the dynamic model\n",
    "sorted_unique_dates = sorted(list(set(dates)))\n",
    "f = open(corpusname+'-seq.dat','wb')\n",
    "f.write(str(len(clean_abstracts))+'\\n')\n",
    "for date in sorted_unique_dates:\n",
    "    f.write(str(dates.count(date))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1998', u'1998', u'1999', u'2000', u'2000', u'2000', u'2000', u'2000', u'2000', u'2000']\n"
     ]
    }
   ],
   "source": [
    "print dates;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def format_conversion(corpusname):\n",
    "    \n",
    "    from gensim import corpora\n",
    "    \n",
    "    #We start by creating the vocabulary file\n",
    "    dictionary = corpora.Dictionary(\n",
    "        [el for el in line.lower().split()]\n",
    "            for line in open(corpusname+'_corpus.txt'))\n",
    "    #Remove words that appear in less than no_below documents, or in more than\n",
    "    #no_above, and keep at most keep_n most frequent terms\n",
    "    dictionary.filter_extremes(no_below=4, no_above=0.5, keep_n=10000)\n",
    "    #We generate the vocabulary file\n",
    "    with open(corpusname + '_vocab.txt','wb') as f:\n",
    "        [f.write(dictionary[idx]+'\\n') for idx in range(len(dictionary))]\n",
    "\n",
    "    #We create now an iterable corpus (memory friendly)\n",
    "    class MyCorpus(object):\n",
    "        def __iter__(self):\n",
    "            for line in open(corpusname+'_corpus.txt'):\n",
    "                yield dictionary.doc2bow(line.lower().split())\n",
    "    corpus_gensim = MyCorpus()\n",
    "    #And generate the file with the format required by Blei's LDA implementation\n",
    "    with open(corpusname + '-mult.dat','wb') as f:\n",
    "        for docbow in corpus_gensim:\n",
    "            docstr = ' '.join([str(el[0])+':'+str(el[1]) for el in docbow])\n",
    "            f.write(str(len(docbow))+' '+docstr+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "format_conversion(corpusname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ u'paper present multiscal decomposit algorithm unlik standard wavelet transform propos oper linear shift invari central idea obtain shift invari averag align wavelet transform project circular shift signal shown transform obtain linear filter bank'\n",
      " u'give systemat abstract formul imag normal method appli general group imag transform illustr abstract analysi appli hierarchi view transform planar object'\n",
      " u'paper present invari scale linear bright chang invari base differenti therefor local featur rotate invari differenti gaussian oper third order propos implement invari perform analyz simul camera'\n",
      " u'describ simpl effici algorithm generat dilat contour bilevel imag initi part contour extract explain good candid parallel comput code generat remaind algorithm linear natur'\n",
      " u'fractal imag compress culik imag compress zerotre predict code wavelet imag decomposit coeffici succeed onli becaus typic imag compress possess signific degre besid common concept method turn even tight relat point algorithm reduc one techniqu anoth goal present paper demonstr relat paper offer interpret culik imag compress regular imag process term without resort finit state machin similar lofti languag interpret shown algorithm relat fractal imag compress method exact transform culik imag code use transform prove part imag ani zero wavelet coeffici root zerotre branch paper discuss zerotre code coeffici common appli vertic differ layer multiresolut decomposit rather within view interpret lead insight evolut imag compress techniqu causal predict predict wavelet decomposit among causal predict culik method'\n",
      " u'paper present invari gamma correct similar transform invari local featur base differenti implement use deriv gaussian use propos invari represent shown yield improv correl result templat match scenario'\n",
      " u'work deal video index viewpoint analysi compress video consid possibl applic motion analysi move object detect assist move object index summaris video allow imag motion queri propos approach base interest point first result test compar stabil differ type interest point detector compress sequenc'\n",
      " u'paper establish verifi theori prevail wide among imag pattern recognit specialist indirect region match process stabl robust global match process concentr type nois repres clutter outlier occlus imageri demonstr analyz effect concentr nois typic decis make process simplifi two candid vote model theorem establish lower bound critic breakdown point elect decis result match process greater exact bound global match process impli former region process capabl accommod higher level nois latter global process befor result decis overturn present convinc experiment verif support onli theori flag recognit problem presenc local nois also valid conjectur facial recognit problem theorem remain valid decis make process involv import transform princip compon analysi gabor transform'\n",
      " u'bayesian classifi differ attribut valu discuss use four popular dataset uci repositori interest featur network illustr network suitabl classif problem'\n",
      " u'algorithm use letter dataset uci repositori classifi distort raster imag english alphabet contrast rather complex network found produc compar better classif effici complex problem']\n"
     ]
    }
   ],
   "source": [
    "    dictionary = corpora.Dictionary(\n",
    "        [el for el in line.lower().split()]\n",
    "            for line in open(corpusname+'_corpus.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
