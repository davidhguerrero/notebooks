{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis and Topic Modelling\n",
    "\n",
    "Author: Jesús Cid Sueiro\n",
    "\n",
    "Date: 2016/04/03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore some tools for text analysis in python. To do so, first we will import the requested python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named test_helper",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e5144fb0b160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtest_helper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named test_helper"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Required imports\n",
    "from wikitools import wiki\n",
    "from wikitools import category\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "from test_helper import Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open(\"wikiresults_culture.p\", \"rb\"))\n",
    "D = data['D']\n",
    "corpus_bow = data['corpus_bow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('deerwester.dict')\n",
    "corpus_bow = corpora.MmCorpus('deerwester.mm')\n",
    "print dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Analysis\n",
    "\n",
    "The dictionary `D` and the Bag of Words in `corpus_bow` are the key inputs to the topic model algorithms. The topic model algorithms in `gensim` assume that input documents are parameterized using the tf-idf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, tfidf can be used to convert any vector from the old representation (bow integer counts) to the new one (TfIdf real-valued weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "tfidf[doc_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to apply a transformation to a whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply a topic modeling algorithm. Latent Semantic Indexing is provided by `LsiModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Generate a LSI model with 5 topics for `corpus_tfidf` and dictionary `D`. You can check de sintaxis for [gensim.models.LsiModel](https://radimrehurek.com/gensim/models/lsimodel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize an LSI transformation\n",
    "lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "# scode: lsi = <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From LSI, we can check both the topic-tokens matrix and the document-topics matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the topics generated by LSI. An intuitive visualization is provided by the `show_topics` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.show_topics(num_topics=5, num_words=25, log=False, formatted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a more useful representation of topics is as a list of tuples `(token, value)`. This is provided by the `show_topic` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Represent the columns of the topic-token matrix as a series of bar diagrams (one per topic) with the top 25 tokens of each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SORTED TOKEN FREQUENCIES (II):\n",
    "plt.rcdefaults()\n",
    "n_topics = 5\n",
    "n_bins = 25\n",
    "\n",
    "# Example data\n",
    "y_pos = range(n_bins-1, -1, -1)\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = 16, 8  # Set figure size\n",
    "\n",
    "for i in range(n_topics):\n",
    "\n",
    "    ### Plot top 25 tokens for topic i\n",
    "    # Read i-thtopic\n",
    "    # scode: <FILL IN>\n",
    "    topic_i = lsi.show_topic(i, topn=n_bins)\n",
    "    tokens = [t[0] for t in topic_i]\n",
    "    weights = [t[1] for t in topic_i]\n",
    "    \n",
    "    # Plot\n",
    "    # scode: <FILL IN>\n",
    "    plt.subplot(1, n_topics, i+1)\n",
    "    plt.barh(y_pos, weights, align='center', alpha=0.4)\n",
    "    plt.yticks(y_pos, tokens)\n",
    "    plt.xlabel('Top {0} topic weights'.format(n_bins))\n",
    "    plt.title('Topic {0}'.format(i))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI approximates any document as a linear combination of the topic vectors. We can compute the topic weights for any input corpus entered as input to the `lsi` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On real corpora, target dimensionality of\n",
    "# 200–500 is recommended as a “golden standard”\n",
    "# Create a double wrapper over the original \n",
    "# corpus bow  tfidf  fold-in-lsi\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "print corpus_lsi[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Find the document with the largest positive weight for topic 0. Compare the document and the topic.\n",
    "\n",
    "- [x] La comparacion realizada es por simple LDA, no he comparado el documento entero :exclamation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract weights from corpus_lsi\n",
    "# scode: weight0 = <FILL IN>\n",
    "tfidf = gensim.models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = tfidf[corpus_bow]\n",
    "lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=1) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "lsi.show_topics(num_topics=1, num_words=25, log=False, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the transformer to topic 0\n",
    "vec_lsi = list ()\n",
    "id_list = 0\n",
    "for text_vec in corpus_bow:\n",
    "    vec_lsi.extend(lsi[text_vec])\n",
    "\n",
    "nmax = max(vec_lsi,key=itemgetter(1))[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejorar esta forma de encontrar el elemento que tiene el máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for v in vec_lsi:\n",
    "    if v[1] == nmax:\n",
    "        index = id_list\n",
    "    id_list +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Locate the maximum positive weight\n",
    "# nmax = np.argmax(weight0)\n",
    "print nmax\n",
    "print weight0[nmax]\n",
    "print corpus_lsi[nmax]\n",
    "\n",
    "# Get topic 0\n",
    "# scode: topic_0 = <FILL IN>\n",
    "\n",
    "# Compute a list of tuples (token, wordcount) for all tokens in topic_0, where wordcount is the number of \n",
    "# occurences of the token in the article.\n",
    "# scode: token_counts = <FILL IN>\n",
    "\n",
    "print \"Topic 0 is:\"\n",
    "print topic_0\n",
    "print \"Token counts:\"\n",
    "print token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several implementations of the LDA topic model in python:\n",
    "\n",
    "* Python library `lda`.\n",
    "* Gensim module: `gensim.models.ldamodel.LdaModel`\n",
    "* Sci-kit Learn module: `sklearn.decomposition`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. LDA using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of the LDA module in `gensim` is similar to LSI. Furthermore, it assumes that a `tf-idf` parametrization is used as an input, which is not in complete agreement with the theoretical model, which assumes documents represented as vectors of token-counts.\n",
    "\n",
    "To use LDA in gensim, we must first create a lda model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldag = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus_tfidf, id2word=dictionary, num_topics=10, update_every=1, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldag.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. LDA using Sci-kit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input matrix to the `sklearn` implementation of LDA contains the token-counts for all documents in the corpus.\n",
    "`sklearn` contains a powerfull `CountVectorizer` method that can be used to construct the input matrix from the `corpus_bow`. \n",
    "\n",
    "First, we will define an auxiliary function to print the top tokens in the model, that has been taken from the `sklearn` documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from an example in sklearn site \n",
    "# http://scikit-learn.org/dev/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "\n",
    "# You can try also with the dataset provided by sklearn in \n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                              remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "\n",
    "    \" \".join(ListaTokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a dataset to feed the Count_Vectorizer object, by joining all tokens in `corpus_clean` in a single string, using a space ' ' as separator.\n",
    "\n",
    "**Task**: Join all tokens from each document in a single string, using a white space as separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open(\"wikiresults.corpus_cleam.p\", \"U\")) ##Add a 'U' to mode to open the file for input with universal newline\n",
    "corpus_clean = data['corpus_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Document 0:\n",
      "use social scienc natur use use mdi 2015 multipl imag align right direct vertic width 220 image1 caption1 human symbol ic express develop prehistor human reach behavior modern image2 caption2 religion ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "# scode: data_samples = <FILL IN>   # Usar join sobre corpus_clean.\n",
    "data_samples = [\" \".join(doc) for doc in corpus_clean]\n",
    "data_samples = map(lambda x: \" \".join(x), corpus_clean)\n",
    "\n",
    "print 'Document 0:'\n",
    "print data_samples[0][0:200], '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to compute the token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.589s.\n",
      "  (0, 953)\t20\n",
      "  (0, 856)\t37\n",
      "  (0, 816)\t14\n",
      "  (0, 631)\t6\n",
      "  (0, 41)\t3\n",
      "  (0, 620)\t1\n",
      "  (0, 478)\t2\n",
      "  (0, 798)\t1\n",
      "  (0, 301)\t2\n",
      "  (0, 470)\t33\n",
      "  (0, 895)\t8\n",
      "  (0, 377)\t12\n",
      "  (0, 296)\t20\n",
      "  (0, 142)\t7\n",
      "  (0, 613)\t11\n",
      "  (0, 778)\t8\n",
      "  (0, 106)\t12\n",
      "  (0, 480)\t5\n",
      "  (0, 111)\t8\n",
      "  (0, 266)\t220\n",
      "  (0, 800)\t3\n",
      "  (0, 678)\t3\n",
      "  (0, 242)\t4\n",
      "  (0, 403)\t5\n",
      "  (0, 709)\t8\n",
      "  :\t:\n",
      "  (0, 503)\t1\n",
      "  (0, 688)\t1\n",
      "  (0, 22)\t2\n",
      "  (0, 14)\t1\n",
      "  (0, 657)\t1\n",
      "  (0, 32)\t2\n",
      "  (0, 286)\t1\n",
      "  (0, 215)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 527)\t1\n",
      "  (0, 323)\t1\n",
      "  (0, 28)\t3\n",
      "  (0, 279)\t1\n",
      "  (0, 273)\t1\n",
      "  (0, 392)\t1\n",
      "  (0, 44)\t1\n",
      "  (0, 980)\t1\n",
      "  (0, 530)\t1\n",
      "  (0, 107)\t1\n",
      "  (0, 923)\t1\n",
      "  (0, 559)\t1\n",
      "  (0, 928)\t1\n",
      "  (0, 206)\t1\n",
      "  (0, 713)\t1\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "n_features = 1000\n",
    "#max_features only consider the top max_features ordered by term frequency.\n",
    "#min_df ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "#max_df ignore terms that have a document frequency strictly higher than the given threshold \n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print tf[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the LDA algorithm. \n",
    "\n",
    "**Task**: Create an LDA object with the following parameters: \n",
    "    n_topics=n_topics, max_iter=5,\n",
    "    learning_method='online',\n",
    "    learning_offset=50.,\n",
    "    random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "# scode: lda = <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Fit model `lda` with the token frequencies computed by `tf_vectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.003s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "corpus_lda = lda.fit_transform(tf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "ref cite book cultur right sea human use web 2016 world file press includ plant univers time polit journal youth\n",
      "Topic #1:\n",
      "right human ref cite intern http nation law unit cultur state univers polit declar econom social welfar includ web convent\n",
      "Topic #2:\n",
      "librari public ref religion cite branch right human york cultur use new state religi 2013 http countri area everi book\n",
      "Topic #3:\n",
      "religion ref religi bread cite bird http peopl cultur christian world includ studi book tradit practic belief use god law\n",
      "Topic #4:\n",
      "film art ref cinema director cite use movi new explor direct critic mainstream televis time stori releas genr man narrat\n",
      "Topic #5:\n",
      "music civil ref cultur perform use common cite form histori includ world classic song new year instrument peopl note mani\n",
      "Topic #6:\n",
      "cultur valu individu ref social legal memori societi peopl law human empower understand differ polit group collectiv studi use mean\n",
      "Topic #7:\n",
      "cultur ndash studi social art ref http scienc use new cite high theori work human classic anthropolog design outlin societi\n",
      "Topic #8:\n",
      "film ref music religion cite art religi includ perform centuri new histori book social cultur world file explor direct key\n",
      "Topic #9:\n",
      "languag educ cultur ref polici lo nation cite music australia research social literaci australian plan http intern 2016 govern civil\n",
      "()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'ListaTokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3e3619e6fc08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopics in LDA model:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf_feature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_feature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-01ee8d7ed84b>\u001b[0m in \u001b[0;36mprint_top_words\u001b[0;34m(model, feature_names, n_top_words)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mListaTokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 'ListaTokens' is not defined"
     ]
    }
   ],
   "source": [
    "n_top_words = 20\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Represent graphically the topic distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explore the influence of the concentration parameters, $alpha$ (`doc_topic_prior` in `sklearn`) and $eta$(`topic_word_prior`). In particular observe how do topic and document distributions change as these parameters increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: The token dictionary and the token distribution have shown that:\n",
    "\n",
    "1. Some tokens, despite being very frequent in the corpus, have no semantic relevance for topic modeling. Unfortunately, they were not present in the stopword list, and have not been elliminated before the analysis.\n",
    "\n",
    "2. A large portion of tokens appear only once and, thus, they are not statistically relevant for the inference engine of the topic models.\n",
    "\n",
    "Revise the entire corpus be removing from the corpus all these sets of terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: Note that we have not used the terms in the article titles, though the can be expected to containg relevant words for the topic modeling. Include the title words in the analyisis. In order to give them a special relevante, insert them in the corpus several time, so as to make their words more significant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise**: The topic modelling algorithms we have tested in this notebook are non-supervised. This makes them difficult to evaluate objectivelly. In order to test if LDA captures real topics, construct a dataset as the mixture of wikipedia articles from 4 different categories, and test if LDA with 4 topics identifies topics closely related to the original categories."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
